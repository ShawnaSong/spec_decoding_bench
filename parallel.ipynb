{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b94176f",
   "metadata": {},
   "source": [
    "# VLLM Parallel Inference Demo\n",
    "This notebook demonstrates how to use VLLM with tensor and pipeline parallelism for efficient model inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cda897",
   "metadata": {},
   "source": [
    "## Setup and Installation\n",
    "First, let's install VLLM and set up the required dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d9aad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install vllm torch transformers accelerate\n",
    "!pip install pandas matplotlib  # For benchmarking visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754ad8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set CUDA environment variables for optimal performance\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"  # Adjust based on available GPUs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b61d7fa",
   "metadata": {},
   "source": [
    "## Initialize VLLM with Model\n",
    "We'll use VLLM's LLM class to load and configure the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2571e664",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "import torch\n",
    "\n",
    "# Initialize the model with basic configuration\n",
    "model_name = \"deepseek-ai/DeepSeek-V2-Lite\"  # You can replace with your preferred model\n",
    "llm = LLM(model=model_name, \n",
    "          trust_remote_code=True,\n",
    "          dtype=\"bfloat16\",\n",
    "          gpu_memory_utilization=0.85)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cd7b37",
   "metadata": {},
   "source": [
    "## Configure Tensor Parallelism\n",
    "Set up tensor parallel inference across multiple GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e7e741",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with tensor parallelism\n",
    "tensor_parallel_llm = LLM(\n",
    "    model=model_name,\n",
    "    tensor_parallel_size=2,  # Number of GPUs for tensor parallelism\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6a2720",
   "metadata": {},
   "source": [
    "## Configure Pipeline Parallelism\n",
    "Implement pipeline parallelism by splitting the model across GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de23eb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model with pipeline parallelism\n",
    "pipeline_parallel_llm = LLM(\n",
    "    model=model_name,\n",
    "    pipeline_parallel_size=2,  # Number of pipeline stages\n",
    "    trust_remote_code=True,\n",
    "    dtype=\"bfloat16\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0763763a",
   "metadata": {},
   "source": [
    "## Sample Text Generation\n",
    "Let's test the model with different parallel configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befb63da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample prompt for testing\n",
    "prompt = \"An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors. The output is\"\n",
    "\n",
    "# Configure sampling parameters\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    max_tokens=100\n",
    ")\n",
    "\n",
    "# Generate with different configurations\n",
    "result_tensor = tensor_parallel_llm.generate([prompt], sampling_params)\n",
    "result_pipeline = pipeline_parallel_llm.generate([prompt], sampling_params)\n",
    "\n",
    "print(\"Tensor Parallel Output:\")\n",
    "print(result_tensor[0].outputs[0].text)\n",
    "print(\"\\nPipeline Parallel Output:\")\n",
    "print(result_pipeline[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5076b8c",
   "metadata": {},
   "source": [
    "## Benchmarking Different Parallel Configurations\n",
    "Compare performance across different parallel setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a87746",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def benchmark_generation(llm, prompt, n_runs=5):\n",
    "    times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = llm.generate([prompt], sampling_params)\n",
    "        times.append(time.time() - start)\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "# Benchmark different configurations\n",
    "configs = {\n",
    "    'Base': llm,\n",
    "    'Tensor Parallel': tensor_parallel_llm,\n",
    "    'Pipeline Parallel': pipeline_parallel_llm\n",
    "}\n",
    "\n",
    "results = {}\n",
    "for name, model in configs.items():\n",
    "    avg_time = benchmark_generation(model, prompt)\n",
    "    results[name] = avg_time\n",
    "\n",
    "# Create visualization\n",
    "df = pd.DataFrame(list(results.items()), columns=['Configuration', 'Time (s)'])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(df['Configuration'], df['Time (s)'])\n",
    "plt.title('Inference Time Comparison')\n",
    "plt.ylabel('Average Time (seconds)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
